{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# linear algebra \n",
    "import numpy as np   \n",
    "# data processing, CSV file I / O (e.g. pd.read_csv) \n",
    "import string\n",
    "import pandas as pd   \n",
    "import os \n",
    "import tensorflow as tf \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.models import Model \n",
    "from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation \n",
    "from keras.layers import concatenate,add, BatchNormalization, Input\n",
    "\n",
    "from keras.utils import to_categorical, plot_model \n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input \n",
    "import matplotlib.pyplot as plt  # for plotting data \n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"
     ]
    }
   ],
   "source": [
    "def load_description(text): \n",
    "\tmapping = dict() \n",
    "\tfor line in text.split(\"\\n\"): \n",
    "\t\ttoken = line.split(\"\\t\") \n",
    "\t\tif len(line) < 2: # remove short descriptions \n",
    "\t\t\tcontinue\n",
    "\t\timg_id = token[0].split('.')[0] # name of the image \n",
    "\t\timg_des = token[1]\t\t\t # description of the image \n",
    "\t\tif img_id not in mapping: \n",
    "\t\t\tmapping[img_id] = list() \n",
    "\t\tmapping[img_id].append(img_des) \n",
    "\treturn mapping \n",
    "\n",
    "token_path = r\"C:\\Users\\shanza\\Downloads\\image dataset\\Flickr_Data\\Flickr_Data\\Flickr_TextData\\Flickr8k.token.txt\"\n",
    "text = open(token_path, 'r', encoding = 'utf-8').read() \n",
    "descriptions = load_description(text) \n",
    "print(descriptions['1000268201_693b08cb0e'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_description(desc): \n",
    "\tfor key, des_list in desc.items(): \n",
    "\t\tfor i in range(len(des_list)): \n",
    "\t\t\tcaption = des_list[i] \n",
    "\t\t\tcaption = [ch for ch in caption if ch not in string.punctuation] \n",
    "\t\t\tcaption = ''.join(caption) \n",
    "\t\t\tcaption = caption.split(' ') \n",
    "\t\t\tcaption = [word.lower() for word in caption if len(word)>1 and word.isalpha()] \n",
    "\t\t\tcaption = ' '.join(caption) \n",
    "\t\t\tdes_list[i] = caption \n",
    "\n",
    "clean_description(descriptions) \n",
    "descriptions['1000268201_693b08cb0e']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(desc): \n",
    "\twords = set() \n",
    "\tfor key in desc.keys(): \n",
    "\t\tfor line in desc[key]: \n",
    "\t\t\twords.update(line.split()) \n",
    "\treturn words \n",
    "vocab = to_vocab(descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startseq child in pink dress is climbing up set of stairs in an entry way endseq', 'startseq girl going into wooden building endseq', 'startseq little girl climbing into wooden playhouse endseq', 'startseq little girl climbing the stairs to her playhouse endseq', 'startseq little girl in pink dress going into wooden cabin endseq']\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "images = r'C:\\Users\\shanza\\Downloads\\image dataset\\Flickr_Data\\Flickr_Data\\Images'\n",
    "# Create a list of all image names in the directory \n",
    "img = glob.glob(images + '*.jpg') \n",
    "\n",
    "train_path = r\"C:\\Users\\shanza\\Downloads\\image dataset\\Flickr_Data\\Flickr_Data\\Flickr_TextData\\Flickr_8k.trainImages.txt\"\n",
    "train_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\") \n",
    "train_img = [] # list of all images in training set \n",
    "for im in img: \n",
    "\tif(im[len(images):] in train_images): \n",
    "\t\ttrain_img.append(im) \n",
    "\t\t\n",
    "# load descriptions of training set in a dictionary. Name of the image will act as ey \n",
    "def load_clean_descriptions(des, dataset): \n",
    "\tdataset_des = dict() \n",
    "\tfor key, des_list in des.items(): \n",
    "\t\tif key+'.jpg' in dataset: \n",
    "\t\t\tif key not in dataset_des: \n",
    "\t\t\t\tdataset_des[key] = list() \n",
    "\t\t\tfor line in des_list: \n",
    "\t\t\t\tdesc = 'startseq ' + line + ' endseq'\n",
    "\t\t\t\tdataset_des[key].append(desc) \n",
    "\treturn dataset_des \n",
    "\n",
    "train_descriptions = load_clean_descriptions(descriptions, train_images) \n",
    "print(train_descriptions['1000268201_693b08cb0e'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array \n",
    "def preprocess_img(img_path): \n",
    "\t# inception v3 excepts img in 299 * 299 * 3 \n",
    "\timg = load_img(img_path, target_size = (299, 299)) \n",
    "\tx = img_to_array(img) \n",
    "\t# Add one more dimension \n",
    "\tx = np.expand_dims(x, axis = 0) \n",
    "\tx = preprocess_input(x) \n",
    "\treturn x \n",
    "\n",
    "def encode(image): \n",
    "\timage = preprocess_img(image) \n",
    "\tvec = model.predict(image) \n",
    "\tvec = np.reshape(vec, (vec.shape[1])) \n",
    "\treturn vec \n",
    "\n",
    "base_model = InceptionV3(weights = 'imagenet') \n",
    "model = Model(base_model.input, base_model.layers[-2].output) \n",
    "# run the encode function on all train images and store the feature vectors in a list \n",
    "encoding_train = {} \n",
    "for img in train_img: \n",
    "\tencoding_train[img[len(images):]] = encode(img) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all training captions \n",
    "all_train_captions = [] \n",
    "for key, val in train_descriptions.items(): \n",
    "\tfor caption in val: \n",
    "\t\tall_train_captions.append(caption) \n",
    "\n",
    "# consider only words which occur atleast 10 times \n",
    "vocabulary = vocab \n",
    "threshold = 10 # you can change this value according to your need \n",
    "word_counts = {} \n",
    "for cap in all_train_captions: \n",
    "\tfor word in cap.split(' '): \n",
    "\t\tword_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "vocab = [word for word in word_counts if word_counts[word] >= threshold] \n",
    "\n",
    "# word mapping to integers \n",
    "ixtoword = {} \n",
    "wordtoix = {} \n",
    "\n",
    "ix = 1\n",
    "for word in vocab: \n",
    "\twordtoix[word] = ix \n",
    "\tixtoword[ix] = word \n",
    "\tix += 1\n",
    "\t\n",
    "# find the maximum length of a description in a dataset \n",
    "max_length = max(len(des.split()) for des in all_train_captions) \n",
    "max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "def load_image_features(img_path):\n",
    "    return np.zeros((299))  # Placeholder, replace with actual image feature extraction\n",
    "\n",
    "X1, X2, y = list(), list(), list()\n",
    "# Create a mapping from words to indices with contiguous indices\n",
    "wordtoix = {word: idx for idx, word in enumerate(word)}\n",
    "\n",
    "vocab_size = len(wordtoix)\n",
    "\n",
    "\n",
    "\n",
    "for key, des_list in train_descriptions.items():\n",
    "    pic = load_image_features(key + '.jpg')\n",
    "\n",
    "    for cap in des_list:\n",
    "        seq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "\n",
    "            # Convert in_seq and out_seq to NumPy arrays\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X1.append(pic)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "\n",
    "X1 = np.array(X1)\n",
    "X2 = np.array(X2)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 200)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def binary_to_float(binary):\n",
    "    try:\n",
    "        result = struct.unpack('!f', binary)[0]\n",
    "    except struct.error:\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "# Assuming you have the 'wordtoix' and 'vocab_size' defined\n",
    "embeddings_index = {}\n",
    "glove_path = r\"C:\\Users\\shanza\\Downloads\\image dataset\\glove.6B.200d.txt\"\n",
    "\n",
    "glove = open(glove_path, 'r', encoding='utf-8').read()\n",
    "\n",
    "for line in glove.split(\"\\n\"):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    indices = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = indices\n",
    "\n",
    "emb_dim = 200\n",
    "emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "# Corrected loop to iterate over the vocabulary size\n",
    "for word, i in wordtoix.items():\n",
    "    if i < vocab_size:  # Ensure that the index is within bounds\n",
    "        emb_vec = embeddings_index.get(word)\n",
    "        if emb_vec is not None:\n",
    "            emb_matrix[i] = emb_vec\n",
    "\n",
    "emb_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "ip1 = Input(shape = (2048, )) \n",
    "fe1 = Dropout(0.2)(ip1) \n",
    "fe2 = Dense(256, activation = 'relu')(fe1) \n",
    "ip2 = Input(shape = (max_length, )) \n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2) \n",
    "se2 = Dropout(0.2)(se1) \n",
    "se3 = LSTM(256)(se2) \n",
    "decoder1 = add([fe2, se3]) \n",
    "decoder2 = Dense(256, activation = 'relu')(decoder1) \n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2) \n",
    "model = Model(inputs = [ip1, ip2], outputs = outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input data to be non-empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# you can increase the number of epochs for better results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shanza\\miniconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\shanza\\miniconda3\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1319\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_dataset_and_inferred_steps(\n\u001b[0;32m   1315\u001b[0m     strategy, x, steps_per_epoch, class_weight, distribute\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input data to be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input data to be non-empty."
     ]
    }
   ],
   "source": [
    "model.layers[2].set_weights([emb_matrix]) \n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam') \n",
    "model.fit([X1, X2], y, epochs = 50, batch_size = 256) \n",
    "# you can increase the number of epochs for better results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(pic): \n",
    "\tstart = 'startseq'\n",
    "\tfor i in range(max_length): \n",
    "\t\tseq = [wordtoix[word] for word in start.split() if word in wordtoix] \n",
    "\t\tseq = pad_sequences([seq], maxlen = max_length) \n",
    "\t\tyhat = model.predict([pic, seq]) \n",
    "\t\tyhat = np.argmax(yhat) \n",
    "\t\tword = ixtoword[yhat] \n",
    "\t\tstart += ' ' + word \n",
    "\t\tif word == 'endseq': \n",
    "\t\t\tbreak\n",
    "\tfinal = start.split() \n",
    "\tfinal = final[1:-1] \n",
    "\tfinal = ' '.join(final) \n",
    "\treturn final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
